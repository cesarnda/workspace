\documentclass[tikz, 12pt]{scrartcl}
\usepackage{etex}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage[ruled,vlined, linesnumbered]{algorithm2e}
\usepackage{fancyhdr}
\usepackage{anysize}
\usepackage{vaucanson-g}
\usepackage{amsthm}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Data Structures and Algorithms}
\subtitle{Spring 2014}
\author{Cesar Agustin Garcia Vazquez}
\date{\today}                                           % Activate to display a given date or no date
\allowdisplaybreaks
\begin{document}
\maketitle
\section{Problem Set 08}

\begin{enumerate}
	\item If we restrict the problems we look at, sometimes hard problems like counting the number of independents sets are in a graph become solvable. For instance, consider a graph that is a line on $n$ vertices. (That is, the vertices are labelled 1 to $n$, and there is an edge from 1 to 2, 2 to 3, etc.) How many independent sets are there on a line graph? Also, how many independent sets are there on a cycle of $n$ vertices? (Hint: In this case, we want to express your answer in terms of a family of numbers - like ``For $n$ vertices the number of independent sets is the $n$-th prime.'' And that's not the answer.)\\
	\\
	Similarly, describe how you could quickly compute the number of independent sets on a complete binary tree. (Here, just explain how to compute this number.) Calculate the number of independent sets on a complete binary tree with 127 nodes.\\
	\\
	\textbf{Answer:} For a linear graph, we can see in Table \ref{linearGraph} the independent sets and the number of independent sets for a couple of scenarios. As we increase the number of vertices, we can see that the previous independent sets remain the same.
	
\begin{longtable}{|c|c|c|}
\caption{Independent sets for a linear Graph\label{linearGraph}}\\
\hline
Number of Elements	&	Independent Sets	&	Number of Independent Sets\\
\hline
	0			&	$\{\}$									&	1		\\
	1			&	$\{\}, \{1\}$									&	2		\\
	2			&	$\{\}, \{1\}, \{2\}$								&	3		\\
	3			&	$\{\}, \{1\}, \{2\}, \{3\}, \{1, 3\}$					&	5		\\
\hline
\end{longtable}	 

In Table \ref{advancedLinearGraph} we stop displaying the previous independent sets, and we only consider the independent sets that can be generated with the new vertex added. For what we can see in the tables, the Fibonacci numbers appear as the result of the number of independent sets. If we consider $F_0 = 1$, $F_1 = 1$ and $F_2 = 2$, then we might suggest that for a graph with $n$ vertices, we have $F_{n + 1}$, independent sets.

\begin{longtable}{|c|c|c|}
\caption{Independent sets for a linear Graph\label{advancedLinearGraph}}\\
\hline
Number of Elements	&	Independent Sets	&	Number of Independent Sets\\
\hline
	4			&	$\{4\}, \{1, 4\}, \{2,4\}$				&	8	\\
	5			&	$\{5\}, \{1, 5\}, \{2, 5\}, \{3, 5\}, \{1, 3, 5\}$	&	13 \\
	$\vdots$		&	$\vdots$							&	$\vdots$\\
	$n$			&	$\vdots$							&	$F_{n + 1}	$\\
\hline
\end{longtable}	 

We can see that the base cases satisfied the proposed formula. So we can assume that for $n$ vertices, the number of independent sets is $F_{n + 1}$. When $n + 1$, we have the first $n$ vertices generate $F_{n + 1}$ independent sets, so now we just need to find out how many independent sets are generated when a new vertex is added. This is the same as computing how many independent sets can be generated when we have $n - 1$ vertices and a new is added, i.e., we are able to add the new vertex in all of the previous independent sets, except for the one that contains the element $n$. Again by the induction hypothesis we have that for $n- 1$, there are $F_{n}$ independent sets and hence for $n + 1$ vertices we have $F_{n} + F_{n + 1} = F_{n + 2}$ independent sets.\\
\\
For the case when we have a cycle of $n$ vertices, we use the same technique that in the previous proof. The last element added generates the previous independent sets plus the independent sets generated by considering all of the $n$ elements but 2, the first and the last one.\\
\textit{Claim:} The number of independent sets in a cycle is $F_{n} $.

\begin{longtable}{|c|p{5cm}|c|}
\caption{Independent sets for a linear Graph\label{linearGraph}}\\
\hline
Number of Elements	&	Independent Sets	&	Number of Independent Sets\\
\hline
	0			&	$\{\}$													&	1		\\
	\hline
	1			&	$\{\}$, $\{1\}$												&	2		\\
	\hline
	2			&	$\{\}$, $\{1\}$, $\{2\}$											&	3		\\
	\hline
	3			&	$\{\}$, $\{1\}$, $\{2\}$, $\{3\}$									&	4		\\
	\hline
	4			&	$\{4\}$, $\{2,4\}$, $\{1, 3\}$									&	7	\\
	\hline
	5			&	$\{5\}$, $\{2, 5\}$, $\{3, 5\}$, $\{1, 4\}$							&	11	\\
	\hline
	6			&	$\{6\}$, $\{1, 5\}$, $\{2, 6\}$, $\{3, 6\}$, $\{4, 6\}$, $\{1, 3, 5\}$, $\{2, 4, 6\}$	&	18\\
	\hline
	7			&	$\{7\}$, $\{1, 6\}$, $\{2, 7\}$, $\{3, 7\}$, $\{4, 7\}$, $\{5, 7\}$, $\{2, 4, 6\}$, $\{ 2, 4, 7\}$, $\{3, 5, 7\}$, $\{2, 5, 7\}$, $\{1, 4, 6\}$ 	&  29\\
%	8			&	$\{8\}$, $\{1, 8\}$, $\{2, 8\}$, $\{3, 8\}$, $\{4, 8\}$, $\{5, 8\}$, $\{6,8\}$,  $\{3, 5, 7\}$	& 32\\
\hline
\end{longtable}	 	

For this condition to be valid, we need to redefine the base case to make match to what we have. Then $F_0 = 1$, $F_1 = 2$, $F_2 =3$ and $F_3 = 4$. So we assume that for $n$ vertices in a cycle graph, we have $F_{n - 1} + F_{n- 2}$ independent sets.\\
As the inductive step, for $n + 1$ vertices, for the first $n$ vertices we can generate $F_{n}$ independent sets. For the extra element, we can add the extra element to all the independent sets that are generated by $n-1$ vertices because we have to remove the sets that are generated with the first element. So we have $F_{n -1}$ independent sets. In total for $n + 1$ vertices we have $F_{n} + F_{n - 1} = F_{n + 1}$ independent sets.\\
\\
For a complete binary tree, when we the heigh is 0, then we have an empty set of nodes, which means that we have 1 independent set. When we have heigh 1, then we have 2 independent sets: $\{\}$ and $\{1\}$. Hence the base case is $I(0) = 1$ and $I(1) = 2$. 	\\
\\
When we have a tree with heigh is 2, then we have $2^2 - 1$ nodes, and besides the independent sets that we have with heigh 1, now we have the independent sets $\{2\}$, $\{3\}$ and $\{2, 3\}$. So when we have heigh 2, then we have 5 independent sets. When the tree is of heigh k, the proposed solution is $I(k) = I(k - 1)^2 + I(k - 2)^4$.

On a complete binary tree with 127 nodes, three is of height 8, since $2^{8 - 1} - 1 = 127$. So we just need to compute $I(8)$. Using BigIntegers in Java, we get that the number of independent sets with 127 is shown in equation (\ref{independentSetsInCompleteBinaryTree}).

\begin{equation}\label{independentSetsInCompleteBinaryTree}
258,159,204,435,047,592,104,207,508,169,153,297,050,209,383,336,364,487,461
\end{equation}

	
	\item Consider the problem MAX-$K$-CUT, which is like the MAX CUT algorithm, except that we divide the vertices into $k$ disjoint sets, and we want to maximize the number of edges between sets. Explain how to generalize both the randomized and the local search algorithms for MAX CUT to MAX-$k$-CUT and prove bounds on their performance.\\
\\
\textbf{Answer:} To solve the problem, we first define a cut. Let $G = \{V, E\}$ be a weighted graph where eadh edge $e \in E$ has a weight $w_e$. A cut is a partition of $V$ into two subsets $S$ and $S^{\prime} = V \backslash S$. We simply denote a cut by either one of its subsets. The value of a cut $S$ is computed as in equation (\ref{aCut}).

\begin{equation}\label{aCut}
c(S) = \sum_{\begin{array}{c}(s,s^{\prime})\\ s\in S, s^{\prime} \in S^{\prime} \end{array} }w_{s,s^{\prime}}
\end{equation}

The Max-Cut problem is defined as given a weighted graph $G = \{V, E\}$ where each edge $e \in E$ has a positive integral weight $w_e$, find a cut in $G$ with maximum cut value.\\
\\
For the Max-$k$-Cut problem, we can start with a random solution, which would imply a random partition. The idea to move from one solution to another is to consider moving an element from one partition $V_i$ to another partition $V_j$ where $i \neq j$. We are going to move a vertex $v$ in $V_i$ to $V_j$ as long as the number of edges in $V_i$ does not decrease below the number of edges of $V_j$. Otherwise, we are not optimizing anything, we are exchanging places between sets.\\
\\
The algorithm is going to finish when we are not able to randomly find an element in a random partition that increases the number of edges in another random partition.\\
\\
For this algorithm, we are going to derive the lower bound. That is, the solution is going to be smaller than what is the optimum. If we define as OPT, the optimal value of the problem and  $w(V_i)$, as the sume of the edges whose ends are in $V_i$, then we can also define $w(V_{i,j})$ as the sum of weights of edges that connect a vertex in $V_i$ to a vertex in $V_j$. so we have we can define $C$ as the sum of weights of the cut edges as $\sum_{i=1}^k \sum_{j = i + 1}^{k}w(V_{i,j})$, then we have that ALG + $C = \sum_{e \in E} w_e$.\\
\\
Based on how the algorithm stopped, we have that $\sum_{u \in V_i}w(V_{i} + \{u\}) > \sum_{u \in V_i}w(V_{j} + \{u\})$, and also we have that $\sum_{u \in V_i}w(V_i + \{u\}) = 2 w(V_i)$.\\
Considering that $\sum_{u \in V_i}w(V_j + \{u\}) = w(V_{i,j})$, this yields $2w(V_i) > w(V_{i,j})$. Adding both sides to the inequality ew get that $2 \sum_{i=1}^k \sum_{j = 1}^k w(V_i) >  \sum_{i=1}^k \sum_{j = 1}^k w(V_{i,j})$, due to the fact that   $\sum_{i=1}^k \sum_{j = 1}^k w(V_{i,j}) = 2 \sum_{i=1}^k \sum_{j = i+1}^k w(V_{i,j})$.\\
This leads to $2\sum_{i=1}^k \sum_{j = 1}^k w(V_{i}) > 2 \sum_{i=1}^k \sum_{j = i+1}^k w(V_{i,j})$, and hence we have that $(k -1) \sum_{i = 1}^k w(V_i) > \sum_{i = 1}^k \sum_{j = i +1}^k w(V_{i,j})$. Based on the previous result, we know that the right hand size of the inequality is $C$ and the left hand size is $(k - 1) ALG$. \\
Finally we have that $OPT < C + ALG < ALG + (k - 1)ALG$, from which we get that $OPT< k \cdot ALG$. 


	\item We know that all of NP-Complete reduce to each other. It would be nice if this meant that an approximation for one NP-Hard problem would lead to another. But this is not the case. Consider the case of Minimum Vertex Cover, for which we have a 2-approximation; that is, we are always within a factor of 2 of the optimal solution. We know (from the NP-Completeness notes, which you may want to check) that $C$ is a cover in a graph $G = (V, E)$ if and only if $V - C$ is an independent set in $V$. Explain why this does not yield an approximation algorithm that is within a constant factor of optimal for Maximum Independent Set. That is, show that for any constant $c_1$, there exists a graph for which even if we obtain a 2-approximation of the Minimum Vertex Cover, the corresponding independent set is not within a factor of $c$ of the Maximum Independent Set.\\
	\\
	The Maximum Independent Set problem and the Maximum Clique problem are related in the following way: an independent set of a graph $G$ is a clique in the complement of $G$. (The complement of $G$ is the graph that contains exactly the edges that are not in $G$.) Does an approximation algorithm (that is, an algorithm within a constant factor of the optimal) for the Maximum Clique problem yield an approximation algorithm (within a constant factor of optimal) for Maximum Independent Set?\\
\\
\textbf{Answer:} If we assume that $S$ is an optimal vertex cover and has size $|V| /2$, then the algorithm for minimum vertex cover is only guaranteed to give a vertex of cover size $|V|!$. So the 2-approximation algorithm does not obtain a non-trivial independent set by complementing the approximate vertex cover. 
	


	\item Suppose we have a random walk with boundaries 0 and $n$, starting at position $i$. As mentioned in class, this can model a gambling game, where we start with $i$ dollars and quit when we lose it all or reach $n$ dollars. Let $W_t$ be our winnings after $t$ games, where $W_t$ is defined only until we hit a boundary (at which point we stop). Note that $W_t$ is negative if we are at a position $j$ with $j < i$; that is, we have lost money. If the probability of winning and the probability of losing 1 dollar each game are $1/2$, then with probability $1/2$, $W_{t + 1} = W_t + 1$ and with probability $1/2$, $W_{t+1} = W_t - 1$. Hence
	\begin{equation}
	E[W_{t + 1}] = \frac{1}{2} E[W_{t} + 1] + \frac{1}{2}E[W_{t} - 1] = E[W_{t}],
	\end{equation}
	where we have used the linearity of expectations at the last step. Therefore when the walk reaches a boundary, the expected winnings is $E[W_0] = 0$. We can use this to calculate the probability that we finish with 0 dollars. Let this probability be $p_0$. Then with probability $p_0$ we lost $i$ dollars, and with probability $1 - p_0$ we gain $n - i$ dollars. Hence
	\begin{equation}
	p_0(-i) + (1 - p_0)(n - i) = 0,
	\end{equation}
	from which we find $p_0  = (n - i) / n$.\\
	\\
	Buffy encounters this game in one of her visits to a hell dimension (or, worse, Las Vegas). Unfortunately, because it is a hell dimension, the game is not fair; instead, the probability of losing a dollar each game is $2/3$, and the probability of winning a dollar each game is $1/3$. Each victim starts with $i$ dollars and gets to leave the hell dimension if they reach $n$ dollars before going bankrupt. We wish to extend the above argument to this case. Show that $E[2^{W_{t+1}}] = E[2^{W_t}]$. Use this to determine the probability of finishing with 0 dollars and the probability of finishing with $n$ dollars when starting at position $i$. Then generalize the argument to handle the case where the probability of losing is $p > 1/2$? Hint: Try using $E[c^{W_t}]$ for some constant $c$.\\
\\
\textbf{Answer:} We first just need to consider the case when $W_t = 0$ and $W_t = n$. When either of these two cases occur,  we have that $W_{t+1} = W_t$ and $E[2^{W_{t+1}}] = E[2^{W_t}]$. The remaining scenario is when $0 < W_t < n$, which implies that $E[2^{W_{t+1}}| W_t] = \frac{2}{3}E[2^{W_{t-1}}]+\frac{1}{3}E[2^{W_t} + 1]$. The by the previous result we have that $ \frac{2}{3}E[2^{W_{t-1}}]+\frac{1}{3}E[2^{W_t} + 1] = \frac{2}{3}E[2^{W_t}] + \frac{1}{3}E[2^{W_t}]$, which implies that $E[2^{W_{t+1}}] = E[2^{W_t}]$.\\
\\
For the general case when the $p$ is the probability of finishing with $n$ dollars and $1- p$ the probability of finishing with 0. Let T be the number of gamblings that we have to do before we stop, either with 0 dollars or $n$ dollars. Considering that $E[2^{W_T}] = E[2^{W_0}]$, then we have that $p \cdot 2^n + (1 - p)2^0 = 2^i$, and then we have that $p(2^n -1) = 2^i - 1$, so we finally reach that the probability of finishing  with $n$ dollars starting at position $i$ is $\frac{2^i - 1}{2^n - 1}$.


\end{enumerate}
%\subsection{}



\end{document}  
